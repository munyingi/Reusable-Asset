{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Training and Evaluation\n",
        "\n",
        "This notebook demonstrates model development, training, evaluation, and business impact analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import joblib\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (classification_report, confusion_matrix, roc_auc_score,\n",
        "                             roc_curve, precision_recall_curve, f1_score)\n",
        "from sklearn.utils import resample\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print('Libraries imported successfully!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Processed Data\n",
        "\n",
        "Load the engineered features from the previous notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data\n",
        "data_path = Path('../data/processed/customer_data_features.csv')\n",
        "\n",
        "if not data_path.exists():\n",
        "    print('Error: Processed data not found. Please run 02-feature-engineering.ipynb first.')\n",
        "    # Generate sample data for demonstration\n",
        "    np.random.seed(42)\n",
        "    n_samples = 1000\n",
        "    data = pd.DataFrame({\n",
        "        'feature_1': np.random.randn(n_samples),\n",
        "        'feature_2': np.random.randn(n_samples),\n",
        "        'feature_3': np.random.randn(n_samples),\n",
        "        'feature_4': np.random.randn(n_samples),\n",
        "        'feature_5': np.random.randn(n_samples),\n",
        "        'churn': np.random.choice([0, 1], n_samples, p=[0.73, 0.27])\n",
        "    })\n",
        "else:\n",
        "    data = pd.read_csv(data_path)\n",
        "\n",
        "print(f'Dataset shape: {data.shape}')\n",
        "print(f'Churn rate: {data[\"churn\"].mean():.2%}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Prepare Training and Test Sets\n",
        "\n",
        "Split data with stratification to maintain class balance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Separate features and target\n",
        "X = data.drop(['churn', 'customer_id'], axis=1, errors='ignore')\n",
        "y = data['churn']\n",
        "\n",
        "# Train-test split with stratification\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f'Training set: {X_train.shape}')\n",
        "print(f'Test set: {X_test.shape}')\n",
        "print(f'\\nClass distribution in training set:')\n",
        "print(y_train.value_counts(normalize=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Handle Class Imbalance\n",
        "\n",
        "Address imbalanced classes using resampling techniques."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Separate majority and minority classes\n",
        "X_train_df = pd.DataFrame(X_train, columns=X.columns)\n",
        "X_train_df['churn'] = y_train.values\n",
        "\n",
        "majority = X_train_df[X_train_df['churn'] == 0]\n",
        "minority = X_train_df[X_train_df['churn'] == 1]\n",
        "\n",
        "# Upsample minority class\n",
        "minority_upsampled = resample(minority,\n",
        "                              replace=True,\n",
        "                              n_samples=len(majority),\n",
        "                              random_state=42)\n",
        "\n",
        "# Combine majority and upsampled minority\n",
        "X_train_balanced = pd.concat([majority, minority_upsampled])\n",
        "\n",
        "# Separate features and target\n",
        "y_train_balanced = X_train_balanced['churn']\n",
        "X_train_balanced = X_train_balanced.drop('churn', axis=1)\n",
        "\n",
        "print(f'Balanced training set: {X_train_balanced.shape}')\n",
        "print(f'\\nClass distribution after balancing:')\n",
        "print(y_train_balanced.value_counts(normalize=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Train Multiple Models\n",
        "\n",
        "Train and compare different algorithms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
        "}\n",
        "\n",
        "# Train and evaluate models\n",
        "results = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f'\\nTraining {name}...')\n",
        "    \n",
        "    # Train model\n",
        "    model.fit(X_train_balanced, y_train_balanced)\n",
        "    \n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "    \n",
        "    # Calculate metrics\n",
        "    results[name] = {\n",
        "        'model': model,\n",
        "        'y_pred': y_pred,\n",
        "        'y_pred_proba': y_pred_proba,\n",
        "        'roc_auc': roc_auc_score(y_test, y_pred_proba),\n",
        "        'f1': f1_score(y_test, y_pred)\n",
        "    }\n",
        "    \n",
        "    print(f'{name} - ROC AUC: {results[name][\"roc_auc\"]:.4f}, F1 Score: {results[name][\"f1\"]:.4f}')\n",
        "\n",
        "print('\\nModel training completed!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Model Comparison\n",
        "\n",
        "Compare model performance using multiple metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comparison dataframe\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Model': list(results.keys()),\n",
        "    'ROC AUC': [results[m]['roc_auc'] for m in results.keys()],\n",
        "    'F1 Score': [results[m]['f1'] for m in results.keys()]\n",
        "}).sort_values('ROC AUC', ascending=False)\n",
        "\n",
        "print('Model Performance Comparison:')\n",
        "print(comparison_df)\n",
        "\n",
        "# Visualize comparison\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "comparison_df.plot(x='Model', y='ROC AUC', kind='bar', ax=axes[0], color='steelblue', legend=False)\n",
        "axes[0].set_title('ROC AUC Score by Model', fontsize=12, fontweight='bold')\n",
        "axes[0].set_ylabel('ROC AUC')\n",
        "axes[0].set_ylim([0.5, 1.0])\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "comparison_df.plot(x='Model', y='F1 Score', kind='bar', ax=axes[1], color='coral', legend=False)\n",
        "axes[1].set_title('F1 Score by Model', fontsize=12, fontweight='bold')\n",
        "axes[1].set_ylabel('F1 Score')\n",
        "axes[1].set_ylim([0.0, 1.0])\n",
        "axes[1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Select Best Model and Detailed Evaluation\n",
        "\n",
        "Perform comprehensive evaluation of the best performing model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select best model\n",
        "best_model_name = comparison_df.iloc[0]['Model']\n",
        "best_model = results[best_model_name]['model']\n",
        "y_pred = results[best_model_name]['y_pred']\n",
        "y_pred_proba = results[best_model_name]['y_pred_proba']\n",
        "\n",
        "print(f'Best Model: {best_model_name}')\n",
        "print('\\n' + '='*60)\n",
        "print('CLASSIFICATION REPORT')\n",
        "print('='*60)\n",
        "print(classification_report(y_test, y_pred, target_names=['No Churn', 'Churn']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Confusion Matrix Analysis\n",
        "\n",
        "Visualize prediction errors and their types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "            xticklabels=['No Churn', 'Churn'],\n",
        "            yticklabels=['No Churn', 'Churn'])\n",
        "plt.title(f'Confusion Matrix - {best_model_name}', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculate error types\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "print(f'\\nTrue Negatives: {tn}')\n",
        "print(f'False Positives: {fp}')\n",
        "print(f'False Negatives: {fn}')\n",
        "print(f'True Positives: {tp}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. ROC Curve and Precision-Recall Curve\n",
        "\n",
        "Analyze model performance across different thresholds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate curves\n",
        "fpr, tpr, roc_thresholds = roc_curve(y_test, y_pred_proba)\n",
        "precision, recall, pr_thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# ROC Curve\n",
        "axes[0].plot(fpr, tpr, color='darkorange', lw=2, \n",
        "             label=f'ROC curve (AUC = {results[best_model_name][\"roc_auc\"]:.3f})')\n",
        "axes[0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n",
        "axes[0].set_xlim([0.0, 1.0])\n",
        "axes[0].set_ylim([0.0, 1.05])\n",
        "axes[0].set_xlabel('False Positive Rate')\n",
        "axes[0].set_ylabel('True Positive Rate')\n",
        "axes[0].set_title('ROC Curve', fontsize=12, fontweight='bold')\n",
        "axes[0].legend(loc='lower right')\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "# Precision-Recall Curve\n",
        "axes[1].plot(recall, precision, color='green', lw=2, label='Precision-Recall curve')\n",
        "axes[1].set_xlim([0.0, 1.0])\n",
        "axes[1].set_ylim([0.0, 1.05])\n",
        "axes[1].set_xlabel('Recall')\n",
        "axes[1].set_ylabel('Precision')\n",
        "axes[1].set_title('Precision-Recall Curve', fontsize=12, fontweight='bold')\n",
        "axes[1].legend(loc='lower left')\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Feature Importance Analysis\n",
        "\n",
        "Identify the most influential features (for tree-based models)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature importance (only for tree-based models)\n",
        "if hasattr(best_model, 'feature_importances_'):\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'feature': X.columns,\n",
        "        'importance': best_model.feature_importances_\n",
        "    }).sort_values('importance', ascending=False)\n",
        "    \n",
        "    # Plot top 15 features\n",
        "    top_n = min(15, len(feature_importance))\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.barh(range(top_n), feature_importance['importance'].head(top_n), color='steelblue')\n",
        "    plt.yticks(range(top_n), feature_importance['feature'].head(top_n))\n",
        "    plt.xlabel('Importance')\n",
        "    plt.title(f'Top {top_n} Feature Importances - {best_model_name}', fontsize=12, fontweight='bold')\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(f'\\nTop {top_n} Most Important Features:')\n",
        "    print(feature_importance.head(top_n))\n",
        "else:\n",
        "    print('Feature importance not available for this model type.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Business Impact Analysis\n",
        "\n",
        "Translate technical metrics into business value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define business costs\n",
        "customer_lifetime_value = 1000  # Value of retaining a customer\n",
        "retention_campaign_cost = 50     # Cost of retention campaign\n",
        "\n",
        "# Calculate business metrics\n",
        "true_positives = tp\n",
        "false_positives = fp\n",
        "false_negatives = fn\n",
        "\n",
        "# Financial impact\n",
        "revenue_saved = true_positives * customer_lifetime_value\n",
        "campaign_cost = (true_positives + false_positives) * retention_campaign_cost\n",
        "missed_revenue = false_negatives * customer_lifetime_value\n",
        "net_benefit = revenue_saved - campaign_cost\n",
        "\n",
        "# ROI calculation\n",
        "roi = (net_benefit / campaign_cost) * 100 if campaign_cost > 0 else 0\n",
        "\n",
        "print('='*60)\n",
        "print('BUSINESS IMPACT ANALYSIS')\n",
        "print('='*60)\n",
        "print(f'\\nCustomers correctly identified as churning: {true_positives}')\n",
        "print(f'Revenue saved from retention: ${revenue_saved:,.0f}')\n",
        "print(f'\\nTotal retention campaigns sent: {true_positives + false_positives}')\n",
        "print(f'Total campaign cost: ${campaign_cost:,.0f}')\n",
        "print(f'\\nCustomers missed (false negatives): {false_negatives}')\n",
        "print(f'Potential revenue loss: ${missed_revenue:,.0f}')\n",
        "print(f'\\n' + '-'*60)\n",
        "print(f'NET BENEFIT: ${net_benefit:,.0f}')\n",
        "print(f'ROI: {roi:.1f}%')\n",
        "print('='*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Model Hyperparameter Tuning\n",
        "\n",
        "Optimize the best model using grid search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define parameter grid (example for Random Forest)\n",
        "if best_model_name == 'Random Forest':\n",
        "    param_grid = {\n",
        "        'n_estimators': [100, 200],\n",
        "        'max_depth': [10, 20, None],\n",
        "        'min_samples_split': [2, 5],\n",
        "        'min_samples_leaf': [1, 2]\n",
        "    }\n",
        "    \n",
        "    print('Performing hyperparameter tuning...')\n",
        "    grid_search = GridSearchCV(\n",
        "        RandomForestClassifier(random_state=42, n_jobs=-1),\n",
        "        param_grid,\n",
        "        cv=3,\n",
        "        scoring='roc_auc',\n",
        "        n_jobs=-1,\n",
        "        verbose=1\n",
        "    )\n",
        "    \n",
        "    grid_search.fit(X_train_balanced, y_train_balanced)\n",
        "    \n",
        "    print(f'\\nBest parameters: {grid_search.best_params_}')\n",
        "    print(f'Best cross-validation score: {grid_search.best_score_:.4f}')\n",
        "    \n",
        "    # Evaluate tuned model\n",
        "    tuned_model = grid_search.best_estimator_\n",
        "    y_pred_tuned = tuned_model.predict(X_test)\n",
        "    y_pred_proba_tuned = tuned_model.predict_proba(X_test)[:, 1]\n",
        "    \n",
        "    print(f'\\nTuned model ROC AUC: {roc_auc_score(y_test, y_pred_proba_tuned):.4f}')\n",
        "    print(f'Tuned model F1 Score: {f1_score(y_test, y_pred_tuned):.4f}')\n",
        "    \n",
        "    # Update best model\n",
        "    best_model = tuned_model\n",
        "else:\n",
        "    print('Hyperparameter tuning example shown for Random Forest only.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Save Model\n",
        "\n",
        "Save the trained model for deployment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create models directory\n",
        "models_dir = Path('../models')\n",
        "models_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Save model\n",
        "model_path = models_dir / f'{best_model_name.lower().replace(\" \", \"_\")}_model.pkl'\n",
        "joblib.dump(best_model, model_path)\n",
        "print(f'Model saved to: {model_path}')\n",
        "\n",
        "# Save feature names\n",
        "feature_names_path = models_dir / 'feature_names.txt'\n",
        "with open(feature_names_path, 'w') as f:\n",
        "    f.write('\\n'.join(X.columns))\n",
        "print(f'Feature names saved to: {feature_names_path}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Model Summary\n",
        "\n",
        "Document key findings and recommendations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "summary = f\"\"\"\n",
        "MODEL TRAINING SUMMARY:\n",
        "======================\n",
        "\n",
        "1. Best Model: {best_model_name}\n",
        "   - ROC AUC: {results[best_model_name]['roc_auc']:.4f}\n",
        "   - F1 Score: {results[best_model_name]['f1']:.4f}\n",
        "\n",
        "2. Business Impact:\n",
        "   - Net Benefit: ${net_benefit:,.0f}\n",
        "   - ROI: {roi:.1f}%\n",
        "   - Customers Saved: {true_positives}\n",
        "\n",
        "3. Model Performance:\n",
        "   - True Positives: {tp}\n",
        "   - False Positives: {fp}\n",
        "   - False Negatives: {fn}\n",
        "   - True Negatives: {tn}\n",
        "\n",
        "4. Recommendations:\n",
        "   - Deploy model with threshold optimization for business goals\n",
        "   - Monitor model performance monthly\n",
        "   - Retrain quarterly or when performance degrades\n",
        "   - Implement A/B testing for retention campaigns\n",
        "   - Track actual retention rates vs predictions\n",
        "\n",
        "5. Next Steps:\n",
        "   - Create model card documentation\n",
        "   - Set up monitoring dashboard\n",
        "   - Implement prediction API\n",
        "   - Establish retraining pipeline\n",
        "\"\"\"\n",
        "\n",
        "print(summary)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}